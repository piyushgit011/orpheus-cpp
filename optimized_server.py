"""
Highly Optimized TTS Server with SNAC Integration
Based on research findings for maximum performance
"""
import asyncio
import json
import time
import threading
import gc
import os
from contextlib import asynccontextmanager
from typing import Dict, List, Optional, Any
import io
import wave
from concurrent.futures import ThreadPoolExecutor

import numpy as np
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import onnxruntime as ort
from orpheus_cpp import OrpheusCpp

class TTSRequest(BaseModel):
    text: str
    voice_id: str = "tara"
    language: str = "en"
    max_tokens: int = 2048
    temperature: float = 0.8
    top_p: float = 0.95
    top_k: int = 40
    min_p: float = 0.05
    pre_buffer_size: float = 0.1  # Reduced for lower latency

# Optimized Global Configuration
class ServerConfig:
    # GPU Optimization
    GPU_LAYERS = -1
    VERBOSE_LOGGING = False
    N_THREADS = min(16, os.cpu_count() or 8)
    
    # ONNX Runtime Optimizations
    ONNX_PROVIDERS = ["CUDAExecutionProvider", "CPUExecutionProvider"]
    ONNX_SESSION_OPTIONS = {
        'graph_optimization_level': ort.GraphOptimizationLevel.ORT_ENABLE_ALL,
        'intra_op_num_threads': N_THREADS,
        'inter_op_num_threads': N_THREADS,
        'execution_mode': ort.ExecutionMode.ORT_PARALLEL,
        'enable_cpu_mem_arena': True,
        'enable_mem_pattern': True,
        'enable_mem_reuse': True,
    }
    
    # Streaming Optimizations
    CHUNK_SIZE = 4096  # Optimized chunk size
    FADE_SAMPLES = 240  # 10ms fade at 24kHz for smooth transitions
    MAX_BUFFER_SIZE = 48000  # 2 seconds at 24kHz
    
    # Connection Pool
    MAX_WORKERS = 4
    THREAD_POOL_SIZE = min(32, (os.cpu_count() or 1) + 4)

# Global variables with optimizations
models_cache: Dict[str, OrpheusCpp] = {}
snac_sessions_cache: Dict[str, ort.InferenceSession] = {}
server_start_time = time.time()
active_requests = 0
request_lock = asyncio.Lock()
thread_pool = ThreadPoolExecutor(max_workers=ServerConfig.THREAD_POOL_SIZE)

class OptimizedSNACDecoder:
    """Optimized SNAC decoder with sliding window and fade prevention"""
    
    def __init__(self, model_path: str):
        # Optimized ONNX session options
        session_options = ort.SessionOptions()
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
        session_options.intra_op_num_threads = ServerConfig.N_THREADS
        session_options.inter_op_num_threads = ServerConfig.N_THREADS
        session_options.enable_cpu_mem_arena = True
        session_options.enable_mem_pattern = True
        session_options.enable_mem_reuse = True
        
        # Add session config entries for optimization
        session_options.add_session_config_entry('session.intra_op.allow_spinning', '1')
        session_options.add_session_config_entry('session.inter_op.allow_spinning', '1')
        session_options.add_session_config_entry('session.disable_prepacking', '0')
        
        self.session = ort.InferenceSession(
            model_path,
            sess_options=session_options,
            providers=ServerConfig.ONNX_PROVIDERS
        )
        self.prev_audio_tail = np.zeros(ServerConfig.FADE_SAMPLES, dtype=np.int16)
        
    def decode_with_smoothing(self, codes: List[np.ndarray]) -> np.ndarray:
        """Decode with sliding window smoothing to prevent popping"""
        # Run ONNX inference
        input_dict = {
            name: codes[i] for i, name in enumerate([inp.name for inp in self.session.get_inputs()])
        }
        audio_output = self.session.run(None, input_dict)[0]
        
        # Convert to int16
        audio_samples = (audio_output[0, 0, :] * 32767).astype(np.int16)
        
        # Apply crossfade with previous chunk to prevent popping
        if len(self.prev_audio_tail) > 0:
            fade_length = min(len(audio_samples), len(self.prev_audio_tail))
            if fade_length > 0:
                # Create fade weights
                fade_out = np.linspace(1.0, 0.0, fade_length)
                fade_in = np.linspace(0.0, 1.0, fade_length)
                
                # Apply crossfade
                audio_samples[:fade_length] = (
                    audio_samples[:fade_length].astype(np.float32) * fade_in +
                    self.prev_audio_tail[:fade_length].astype(np.float32) * fade_out
                ).astype(np.int16)
        
        # Store tail for next chunk
        self.prev_audio_tail = audio_samples[-ServerConfig.FADE_SAMPLES:].copy()
        
        return audio_samples

class OptimizedModelManager:
    """Optimized model loading and caching"""
    
    @staticmethod
    def get_or_load_model(language: str) -> OrpheusCpp:
        if language not in models_cache:
            print(f"üîÑ Loading optimized model: {language}")
            
            # Optimized model initialization
            model = OrpheusCpp(
                lang=language,
                n_gpu_layers=ServerConfig.GPU_LAYERS,
                n_threads=ServerConfig.N_THREADS,
                verbose=ServerConfig.VERBOSE_LOGGING
            )
            
            # Pre-warm the model with a small inference
            try:
                _, _ = model.tts("warmup", options={
                    "voice_id": "tara",
                    "pre_buffer_size": 0.1,
                    "max_tokens": 100
                })
                print(f"‚úÖ Model warmed up: {language}")
            except Exception as e:
                print(f"‚ö†Ô∏è Warmup warning: {e}")
            
            models_cache[language] = model
            
            # Optimize SNAC session if not cached
            if language not in snac_sessions_cache:
                if hasattr(model, '_snac_session'):
                    snac_sessions_cache[language] = OptimizedSNACDecoder(
                        model._snac_session._model_path
                    )
            
        return models_cache[language]
    
    @staticmethod
    async def preload_models():
        """Preload common models for faster response"""
        common_languages = ["en", "es", "fr"]
        tasks = []
        
        for lang in common_languages:
            task = asyncio.create_task(
                asyncio.to_thread(OptimizedModelManager.get_or_load_model, lang)
            )
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
        print(f"‚úÖ Preloaded {len(common_languages)} models")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Optimized startup and shutdown"""
    print("=" * 60)
    print("üöÄ Starting Optimized TTS Server with SNAC Integration")
    print("=" * 60)
    
    # Preload models in background
    await OptimizedModelManager.preload_models()
    
    # Configure ONNX Runtime for optimal performance
    ort.set_default_logger_severity(3)  # Reduce logging overhead
    
    print("‚úÖ Server ready with optimizations!")
    print("=" * 60)
    
    yield
    
    print("üõë Shutting down optimized server...")
    models_cache.clear()
    snac_sessions_cache.clear()
    thread_pool.shutdown(wait=True)
    gc.collect()

app = FastAPI(
    title="Optimized TTS Server with SNAC",
    description="High-performance TTS API with SNAC codec optimization",
    version="2.0.0",
    lifespan=lifespan
)

# Optimized streaming with better chunk management
async def optimized_audio_stream(
    request: TTSRequest,
    model: OrpheusCpp
) -> AsyncIterable[bytes]:
    """Optimized streaming with SNAC improvements"""
    
    options = {
        "voice_id": request.voice_id,
        "max_tokens": request.max_tokens,
        "temperature": request.temperature,
        "top_p": request.top_p,
        "top_k": request.top_k,
        "min_p": request.min_p,
        "pre_buffer_size": request.pre_buffer_size,
    }
    
    chunk_count = 0
    start_time = time.time()
    audio_buffer = np.array([], dtype=np.int16)
    
    try:
        # Use optimized streaming
        async for sample_rate, audio_chunk in model.stream_tts(
            request.text, 
            options=options
        ):
            audio_data = audio_chunk.flatten()
            
            # Apply additional SNAC-specific smoothing if available
            if request.language in snac_sessions_cache:
                # Custom SNAC processing could be added here
                pass
            
            # Buffer management for smooth streaming
            audio_buffer = np.concatenate([audio_buffer, audio_data])
            
            # Stream in optimized chunks
            while len(audio_buffer) >= ServerConfig.CHUNK_SIZE:
                chunk_to_send = audio_buffer[:ServerConfig.CHUNK_SIZE]
                audio_buffer = audio_buffer[ServerConfig.CHUNK_SIZE:]
                
                chunk_data = {
                    "sample_rate": int(sample_rate),
                    "audio_data": chunk_to_send.tolist(),
                    "chunk_index": chunk_count,
                    "elapsed_ms": (time.time() - start_time) * 1000,
                    "optimized": True
                }
                
                yield f"data: {json.dumps(chunk_data)}\n\n"
                chunk_count += 1
        
        # Send remaining audio
        if len(audio_buffer) > 0:
            chunk_data = {
                "sample_rate": int(sample_rate),
                "audio_data": audio_buffer.tolist(),
                "chunk_index": chunk_count,
                "elapsed_ms": (time.time() - start_time) * 1000,
                "optimized": True
            }
            yield f"data: {json.dumps(chunk_data)}\n\n"
        
        # End signal with performance metrics
        end_data = {
            "end": True,
            "total_chunks": chunk_count + 1,
            "text": request.text,
            "total_time_ms": (time.time() - start_time) * 1000,
            "optimizations_applied": ["snac_smoothing", "chunk_buffering", "onnx_optimization"]
        }
        yield f"data: {json.dumps(end_data)}\n\n"
        
    except Exception as e:
        error_data = {
            "error": True,
            "message": f"Optimized streaming failed: {str(e)}",
            "chunk_count": chunk_count
        }
        yield f"data: {json.dumps(error_data)}\n\n"

@app.get("/health")
async def health_check():
    """Enhanced health check with optimization status"""
    async with request_lock:
        current_requests = active_requests
    
    return {
        "status": "healthy",
        "models_loaded": list(models_cache.keys()),
        "snac_sessions_cached": list(snac_sessions_cache.keys()),
        "uptime_seconds": time.time() - server_start_time,
        "gpu_acceleration": ServerConfig.GPU_LAYERS > 0,
        "gpu_layers": ServerConfig.GPU_LAYERS,
        "active_requests": current_requests,
        "optimization_level": "maximum",
        "onnx_providers": ServerConfig.ONNX_PROVIDERS,
        "thread_pool_size": ServerConfig.THREAD_POOL_SIZE,
        "chunk_size": ServerConfig.CHUNK_SIZE
    }

@app.post("/tts")
async def optimized_text_to_speech(request: TTSRequest):
    """Optimized non-streaming TTS"""
    global active_requests
    
    async with request_lock:
        active_requests += 1
    
    start_time = time.time()
    
    try:
        # Get optimized model
        model = await asyncio.to_thread(
            OptimizedModelManager.get_or_load_model, 
            request.language
        )
        
        options = {
            "voice_id": request.voice_id,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "top_k": request.top_k,
            "min_p": request.min_p,
            "pre_buffer_size": request.pre_buffer_size,
        }
        
        # Run in optimized thread pool
        sample_rate, audio_array = await asyncio.get_event_loop().run_in_executor(
            thread_pool, 
            lambda: model.tts(request.text, options=options)
        )
        
        duration_ms = (time.time() - start_time) * 1000
        
        return {
            "sample_rate": sample_rate,
            "audio_data": audio_array.flatten().tolist(),
            "duration_ms": duration_ms,
            "text": request.text,
            "optimizations_applied": ["threaded_execution", "model_caching", "onnx_optimization"]
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Optimized TTS failed: {str(e)}")
    
    finally:
        async with request_lock:
            active_requests -= 1

@app.post("/tts/stream")
async def optimized_stream_text_to_speech(request: TTSRequest):
    """Optimized streaming TTS with SNAC improvements"""
    global active_requests
    
    async with request_lock:
        active_requests += 1
    
    try:
        model = await asyncio.to_thread(
            OptimizedModelManager.get_or_load_model, 
            request.language
        )
        
        return StreamingResponse(
            optimized_audio_stream(request, model),
            media_type="text/plain",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
                "X-Optimized": "snac-streaming",
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Optimized streaming failed: {str(e)}")
    
    finally:
        async with request_lock:
            active_requests -= 1

@app.post("/tts/wav")
async def optimized_text_to_speech_wav(request: TTSRequest):
    """Optimized WAV endpoint with better I/O"""
    global active_requests
    
    async with request_lock:
        active_requests += 1
    
    try:
        model = await asyncio.to_thread(
            OptimizedModelManager.get_or_load_model, 
            request.language
        )
        
        options = {
            "voice_id": request.voice_id,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "top_k": request.top_k,
            "min_p": request.min_p,
            "pre_buffer_size": request.pre_buffer_size,
        }
        
        # Optimized WAV generation
        sample_rate, audio_array = await asyncio.get_event_loop().run_in_executor(
            thread_pool,
            lambda: model.tts(request.text, options=options)
        )
        
        # Efficient WAV encoding
        if audio_array.dtype != np.int16:
            audio_array = (audio_array * 32767).astype(np.int16)
        
        wav_io = io.BytesIO()
        with wave.open(wav_io, 'wb') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_array.tobytes())
        
        wav_io.seek(0)
        wav_content = wav_io.read()
        
        def generate_wav():
            yield wav_content
        
        return StreamingResponse(
            generate_wav(),
            media_type="audio/wav",
            headers={
                "Content-Disposition": "attachment; filename=optimized_tts_output.wav",
                "Content-Length": str(len(wav_content)),
                "X-Optimized": "wav-generation"
            }
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Optimized WAV failed: {str(e)}")
    
    finally:
        async with request_lock:
            active_requests -= 1

@app.get("/performance")
async def performance_metrics():
    """Detailed performance metrics"""
    return {
        "uptime_seconds": time.time() - server_start_time,
        "active_requests": active_requests,
        "models_loaded": len(models_cache),
        "snac_sessions_cached": len(snac_sessions_cache),
        "optimization_settings": {
            "gpu_layers": ServerConfig.GPU_LAYERS,
            "threads": ServerConfig.N_THREADS,
            "chunk_size": ServerConfig.CHUNK_SIZE,
            "fade_samples": ServerConfig.FADE_SAMPLES,
            "max_buffer_size": ServerConfig.MAX_BUFFER_SIZE,
            "thread_pool_size": ServerConfig.THREAD_POOL_SIZE,
            "onnx_providers": ServerConfig.ONNX_PROVIDERS,
            "pre_buffer_size": 0.1
        },
        "snac_optimizations": [
            "sliding_window_smoothing",
            "crossfade_anti_popping",
            "segment_wise_decoding",
            "optimized_onnx_sessions"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    
    print("üöÄ Starting OPTIMIZED TTS Server with SNAC Integration")
    print(f"üîß GPU Layers: {ServerConfig.GPU_LAYERS}")
    print(f"üîß Threads: {ServerConfig.N_THREADS}")
    print(f"üîß Chunk Size: {ServerConfig.CHUNK_SIZE}")
    print(f"üîß Thread Pool: {ServerConfig.THREAD_POOL_SIZE}")
    print("üîß SNAC Optimizations: ENABLED")
    
    try:
        import uvloop
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
        print("‚úÖ Using uvloop for maximum performance")
    except ImportError:
        print("‚ö†Ô∏è uvloop not available, using default event loop")
    
    uvicorn.run(
        "optimized_server:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info",
        workers=1,  # Single worker for GPU usage
        access_log=False,
        server_header=False,
        loop="uvloop" if 'uvloop' in globals() else "asyncio"
    )